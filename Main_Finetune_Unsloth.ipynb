{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "kgLpSMBiQQC9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Dataset Preparation Default\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VGDGApiKJG1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Define the Alpaca-style prompt template\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Use tokenizer's EOS token\n",
        "EOS_TOKEN = tokenizer.eos_token  # Make sure this is defined in your environment\n",
        "\n",
        "# Load the dataset from disk\n",
        "dataset = load_from_disk(\"/content/formatted_dataset\")\n",
        "\n",
        "# Check if the data is loaded correctly\n",
        "print(\"Current columns in the dataset:\", dataset.column_names)\n",
        "\n",
        "# Formatting function to map \"text\" if separate instruction, input, and output columns exist\n",
        "def formatting_prompts_func(examples):\n",
        "    # Check if separate columns exist\n",
        "    if \"instruction\" in dataset.column_names and \"input\" in dataset.column_names and \"output\" in dataset.column_names:\n",
        "        instructions = examples[\"instruction\"]\n",
        "        inputs = examples[\"input\"]\n",
        "        outputs = examples[\"output\"]\n",
        "    else:\n",
        "        # If only \"text\" field exists, assume it is already formatted\n",
        "        return examples  # No need to reformat if already in Alpaca style\n",
        "\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Format with Alpaca-style prompt and add EOS_TOKEN\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# View a sample before transformation if separate columns exist\n",
        "if \"instruction\" in dataset.column_names:\n",
        "    print(\"Sample of dataset before transformation:\")\n",
        "    print(dataset.select(range(3)))\n",
        "\n",
        "# Map the formatting function to the dataset if needed\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# View a sample of the dataset after transformation\n",
        "print(\"\\nSample of dataset after transformation:\")\n",
        "print(dataset.select(range(3))[\"text\"])\n"
      ],
      "metadata": {
        "id": "A-qVgzPVNnTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset If Already Formatted The Custom Dataset to Alpaca-Prompt Template**"
      ],
      "metadata": {
        "id": "eB9hinu8JQgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_from_disk(\"/content/formatted_dataset\")\n",
        "\n",
        "# Check column names\n",
        "print(\"Columns in the dataset:\", dataset.column_names)\n",
        "\n",
        "# Preview some data\n",
        "print(\"Sample data:\", dataset.select(range(3)))\n"
      ],
      "metadata": {
        "id": "YlrOZq3XMT3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QUVX4KWUsJdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Three Fine-Tune Approaches and Training Settings**"
      ],
      "metadata": {
        "id": "lxPd7IbMhWeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "import gc\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ======== Configuration ========\n",
        "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B\"\n",
        "DATASET_PATH = \"/content/formatted_dataset\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Reduced for memory efficiency\n",
        "\n",
        "# ======== Load Dataset ========\n",
        "try:\n",
        "    dataset = load_from_disk(DATASET_PATH)\n",
        "    assert \"text\" in dataset.column_names\n",
        "    print(f\"Loaded dataset with {len(dataset)} samples\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Dataset loading failed: {str(e)}\")\n",
        "\n",
        "# ======== Base Model Setup ========\n",
        "def load_base_model():\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    return FastLanguageModel.from_pretrained(\n",
        "        model_name=MODEL_NAME,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=False,\n",
        "        # Remove use_flash_attention_2=True or set attn_implementation='flash_attention_2'\n",
        "        # use_flash_attention_2=True,\n",
        "        attn_implementation=\"flash_attention_2\", # Use this to enable flash attention 2\n",
        "    )\n",
        "\n",
        "# Define tokenizer outside the functions to make it globally accessible\n",
        "model, tokenizer = load_base_model()  # Load the model and tokenizer here\n",
        "\n",
        "# ======== Training Configurations ========\n",
        "def get_training_args(method_name):\n",
        "    return TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_ratio=0.1,\n",
        "        max_steps=5,\n",
        "        learning_rate=5e-5,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        output_dir=f\"./outputs/{method_name}\",\n",
        "        report_to=\"none\",\n",
        "        save_strategy=\"no\",\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "# ======== LoRA Methods ========\n",
        "def train_lora():\n",
        "    # model, tokenizer = load_base_model()  # Remove this line as they are loaded globally now\n",
        "    model_lora = FastLanguageModel.get_peft_model( # Renamed to avoid conflict with global 'model'\n",
        "        model,\n",
        "        r=16,\n",
        "        target_modules=TARGET_MODULES,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=True,\n",
        "        use_rslora=False,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model_lora,  # Use model_lora here\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        tokenizer=tokenizer,  # Now accessible globally\n",
        "        args=get_training_args(\"lora\"),\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    return model_lora, trainer, time.time() - start_time # Return model_lora\n",
        "\n",
        "def train_qlora():\n",
        "    # model, tokenizer = load_base_model()  # Remove this line as they are loaded globally now\n",
        "    model_qlora = FastLanguageModel.get_peft_model( # Renamed to avoid conflict with global 'model'\n",
        "        model,\n",
        "        r=8,  # Lower rank for 4-bit\n",
        "        target_modules=TARGET_MODULES,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=True,\n",
        "        use_rslora=False,  # ✅ REMOVE use_4bit_quantization\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model_qlora,  # Use model_qlora here\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        tokenizer=tokenizer,  # Now accessible globally\n",
        "        args=get_training_args(\"qlora\"),\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    return model_qlora, trainer, time.time() - start_time # Return model_qlora\n",
        "\n",
        "# ======== BitFit Method ========\n",
        "def train_bitfit():\n",
        "    # model, tokenizer = load_base_model()  # Remove this line as they are loaded globally now\n",
        "    model_bitfit = FastLanguageModel.get_peft_model( # Renamed to avoid conflict with global 'model'\n",
        "        model,\n",
        "        r=1,  # Set a positive rank value for get_peft_model\n",
        "        target_modules=TARGET_MODULES,  # Use defined TARGET_MODULES\n",
        "        lora_alpha=1,  # BitFit doesn't require Lora alpha\n",
        "        lora_dropout=0.0,  # No dropout needed for BitFit\n",
        "        bias=\"all\",  # Update all bias parameters\n",
        "        use_gradient_checkpointing=True,\n",
        "        use_rslora=False,  # Not using RS-LoRA in this method\n",
        "    )\n",
        "\n",
        "    # ... (rest of the function remains the same)\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model_bitfit,  # Use model_bitfit here\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        tokenizer=tokenizer,  # Now accessible globally\n",
        "        args=get_training_args(\"bitfit\"),\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    return model_bitfit, trainer, time.time() - start_time # Return model_bitfit\n",
        "\n",
        "# ======== Experiment Runner ========\n",
        "results = {}\n",
        "\n",
        "def run_experiment(method_name, trainer_func):\n",
        "    # Memory cleanup\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    start_time = time.time()\n",
        "\n",
        "    model_exp = None  # Initialize model to None # Renamed to avoid conflict\n",
        "    trainer = None  # Initialize trainer to None\n",
        "\n",
        "    try:\n",
        "        model_exp, trainer, duration = trainer_func()  # Use model_exp to store returned model\n",
        "        peak_mem = torch.cuda.max_memory_reserved()\n",
        "\n",
        "        # Accessing loss from the trainer's state\n",
        "        loss = trainer.state.log_history[-1].get(\"loss\", None)  # Get loss, default to None if not found\n",
        "\n",
        "        # Check if loss is None and handle it, e.g., set a default value\n",
        "        if loss is None:\n",
        "            loss = float('inf')  # Or any other suitable default value like 0\n",
        "\n",
        "        results[method_name] = {\n",
        "            \"time\": duration,\n",
        "            \"memory\": peak_mem,\n",
        "            \"params\": sum(p.numel() for p in model_exp.parameters() if p.requires_grad), # Use model_exp\n",
        "            \"loss\": loss,  # Assign the handled loss value\n",
        "        }\n",
        "    finally:\n",
        "\n",
        "        #FastLanguageModel.save_pretrained_gguf(model, f\"./outputs/{method_name}_gguf\")\n",
        "        model_exp.save_pretrained_gguf(f\"./outputs/{method_name}_gguf\", tokenizer=tokenizer) # Use model_exp & global tokenizer\n",
        "        print(f\"✅ Saved {method_name} model in GGUF format\")\n",
        "\n",
        "        # Only delete if they exist\n",
        "        if model_exp is not None: # Use model_exp\n",
        "            del model_exp\n",
        "        if trainer is not None:\n",
        "            del trainer\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# ======== Execute Experiments ========\n",
        "print(\"=== Running LoRA ===\")\n",
        "run_experiment(\"LoRA\", train_lora)\n",
        "\n",
        "print(\"\\n=== Running QLoRA ===\")\n",
        "run_experiment(\"QLoRA\", train_qlora)\n",
        "\n",
        "print(\"\\n=== Running BitFit ===\")  # Changed from RS-LoRA to BitFit\n",
        "run_experiment(\"BitFit\", train_bitfit)\n",
        "\n",
        "# ======== Results Visualization ========\n",
        "def plot_results(results):\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "    # Training Time\n",
        "    axs[0,0].bar(results.keys(), [v[\"time\"] for v in results.values()], color='skyblue')\n",
        "    axs[0,0].set_title(\"Training Time Comparison\", fontsize=14)\n",
        "    axs[0,0].set_ylabel(\"Seconds\", fontsize=12)\n",
        "\n",
        "    # Memory Usage\n",
        "    axs[0,1].bar(results.keys(), [v[\"memory\"]/1e9 for v in results.values()], color='lightgreen')\n",
        "    axs[0,1].set_title(\"Peak GPU Memory Usage\", fontsize=14)\n",
        "    axs[0,1].set_ylabel(\"GB\", fontsize=12)\n",
        "\n",
        "    # Trainable Parameters\n",
        "    axs[1,0].bar(results.keys(), [v[\"params\"]/1e6 for v in results.values()], color='salmon')\n",
        "    axs[1,0].set_title(\"Trainable Parameters\", fontsize=14)\n",
        "    axs[1,0].set_ylabel(\"Millions\", fontsize=12)\n",
        "\n",
        "    # Training Loss\n",
        "    axs[1,1].bar(results.keys(), [v[\"loss\"] for v in results.values()], color='gold')\n",
        "    axs[1,1].set_title(\"Final Training Loss\", fontsize=14)\n",
        "    axs[1,1].set_ylabel(\"Loss\", fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(results)\n",
        "\n",
        "# ======== Print Numerical Results ========\n",
        "print(\"\\n=== Comparative Results ===\")\n",
        "for method, metrics in results.items():\n",
        "    print(f\"\\n{method}:\")\n",
        "    print(f\"  Training Time: {metrics['time']:.2f}s\")\n",
        "    print(f\"  Peak Memory: {metrics['memory']/1e9:.2f}GB\")\n",
        "    print(f\"  Trainable Params: {metrics['params']/1e6:.2f}M\")\n",
        "    print(f\"  Final Loss: {metrics['loss']:.4f}\")"
      ],
      "metadata": {
        "id": "kv-vuiknhXNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HrM4pTRm9cz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rest of Script are DEMOS, Modify as per convenience**"
      ],
      "metadata": {
        "id": "wpQpMzFh9fDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Direct inference with groundtruth data**"
      ],
      "metadata": {
        "id": "drDSRfd5O_Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the Alpaca-style prompt template\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Load the tokenizer (Tokenized same for all three)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./outputs/BitFit_gguf\")\n",
        "\n",
        "# Define model paths\n",
        "model_paths = {\n",
        "    \"BitFit\": \"./outputs/BitFit_gguf\",\n",
        "    \"LoRA\": \"./outputs/LoRA_gguf\",\n",
        "    \"QLoRA\": \"./outputs/QLoRA_gguf\"\n",
        "}\n",
        "\n",
        "# Load ground truth data from JSON\n",
        "with open(\"ground_truth.json\", \"r\") as f:\n",
        "    ground_truth_data = json.load(f)\n",
        "\n",
        "# Function to generate responses from all models\n",
        "def generate_responses(question):\n",
        "    inputs = tokenizer([alpaca_prompt.format(question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "    model_responses = {}\n",
        "\n",
        "    for model_name, model_path in model_paths.items():\n",
        "        # Load model\n",
        "        model = FastLanguageModel.from_pretrained(model_path).to(\"cuda\")  # Remove torch_dtype=torch.float16\n",
        "        FastLanguageModel.for_inference(model)  # Enable Unsloth optimization\n",
        "\n",
        "        # Generate output\n",
        "        outputs = model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
        "        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        # Store response\n",
        "        model_responses[model_name] = response\n",
        "\n",
        "        # Free GPU memory after inference\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return model_responses\n",
        "\n",
        "# Perform inference on all ground truth prompts\n",
        "results = []\n",
        "for entry in ground_truth_data:\n",
        "    question = entry[\"prompt\"]\n",
        "    ground_truth = entry[\"response\"]\n",
        "\n",
        "    model_responses = generate_responses(question)\n",
        "\n",
        "    results.append({\n",
        "        \"prompt\": question,\n",
        "        \"ground_truth\": ground_truth,\n",
        "        \"model_responses\": model_responses\n",
        "    })\n",
        "\n",
        "# Save results to a JSON file\n",
        "output_file = \"model_comparison_results.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"✅ Results saved in {output_file}\")\n"
      ],
      "metadata": {
        "id": "qlv_1Vkb46Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluations Samples**"
      ],
      "metadata": {
        "id": "cZkYnxRQvPLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "!pip install sacrebleu bert-score pandas seaborn matplotlib\n",
        "!pip install numpy matplotlib seaborn editdistance scikit-learn\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "OtrSH3Ce7Phq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import editdistance\n",
        "from nltk.translate import meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import bert_score\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Function to calculate BLEU score\n",
        "def calculate_bleu(ground_truth, response):\n",
        "    ground_truth_tokens = ground_truth.lower().split()\n",
        "    response_tokens = response.lower().split()\n",
        "\n",
        "    # BLEU score calculation using 1-gram and smoothing\n",
        "    smoothing = SmoothingFunction().method4\n",
        "    bleu_score = sentence_bleu([ground_truth_tokens], response_tokens, smoothing_function=smoothing)\n",
        "    return bleu_score\n",
        "\n",
        "# Function to calculate METEOR score\n",
        "def calculate_meteor(ground_truth, response):\n",
        "    hypothesis_tokens = response.lower().split()\n",
        "    reference_tokens = ground_truth.lower().split()\n",
        "    return meteor_score.single_meteor_score(reference_tokens, hypothesis_tokens)\n",
        "\n",
        "# Function to calculate TER (Translation Edit Rate)\n",
        "def calculate_ter(ground_truth, response):\n",
        "    return editdistance.eval(ground_truth, response) / max(len(ground_truth.split()), 1)\n",
        "\n",
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(ground_truth, response):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = scorer.score(ground_truth, response)\n",
        "    return scores[\"rouge1\"].fmeasure, scores[\"rouge2\"].fmeasure, scores[\"rougeL\"].fmeasure\n",
        "\n",
        "# Function to calculate BERTScore\n",
        "def calculate_bertscore(ground_truth, response):\n",
        "    P, R, F1 = bert_score.score([response], [ground_truth], lang=\"en\")\n",
        "    return F1.item()  # Return the F1 score (semantic similarity)\n",
        "\n",
        "# Function to calculate F1 Score (Quantitative Metric)\n",
        "def calculate_f1(ground_truth, response):\n",
        "    gt_tokens = ground_truth.lower().split()\n",
        "    response_tokens = response.lower().split()\n",
        "\n",
        "    if not gt_tokens or not response_tokens:\n",
        "        return 0.0  # Return 0 if either is empty\n",
        "\n",
        "    # Pad shorter list with 0s to match the length of the longer list\n",
        "    max_len = max(len(gt_tokens), len(response_tokens))\n",
        "    gt_tokens.extend([0] * (max_len - len(gt_tokens)))\n",
        "    response_tokens.extend([0] * (max_len - len(response_tokens)))\n",
        "\n",
        "    common = set(gt_tokens) & set(response_tokens)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    precision = len(common) / max(len(response_tokens), 1)\n",
        "    recall = len(common) / max(len(gt_tokens), 1)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "    return f1\n",
        "\n",
        "# Function to calculate CER (Character Error Rate)\n",
        "def calculate_cer(ground_truth, response):\n",
        "    if not ground_truth or not response:\n",
        "        return 1.0 if ground_truth or response else 0.0\n",
        "    return editdistance.eval(ground_truth, response) / max(len(ground_truth), 1)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_models(data):\n",
        "    results = {}\n",
        "\n",
        "    for group in data[\"groups\"]:\n",
        "        group_name = group[\"group_name\"]\n",
        "        group_results = []\n",
        "\n",
        "        for entry in group[\"questions\"]:\n",
        "            question = entry[\"question\"]\n",
        "            ground_truth = entry[\"groundtruth\"]\n",
        "            responses = entry[\"responses\"]\n",
        "\n",
        "            # Evaluate responses here...\n",
        "            model_results = {}\n",
        "            for model_name, response in responses.items():\n",
        "                bleu = calculate_bleu(ground_truth, response)\n",
        "                meteor = calculate_meteor(ground_truth, response)\n",
        "                ter = calculate_ter(ground_truth, response)\n",
        "                rouge1, rouge2, rougeL = calculate_rouge(ground_truth, response)\n",
        "                bertscore = calculate_bertscore(ground_truth, response)\n",
        "                f1 = calculate_f1(ground_truth, response)\n",
        "                cer = calculate_cer(ground_truth, response)\n",
        "\n",
        "                # Store the model evaluation results\n",
        "                model_results[model_name] = {\n",
        "                    \"f1_score\": f1,\n",
        "                    \"cer\": cer,\n",
        "                    \"bleu_score\": bleu,\n",
        "                    \"meteor_score\": meteor,\n",
        "                    \"ter_score\": ter,\n",
        "                    \"rouge1_score\": rouge1,\n",
        "                    \"rouge2_score\": rouge2,\n",
        "                    \"rougeL_score\": rougeL,\n",
        "                    \"bertscore\": bertscore,\n",
        "                }\n",
        "\n",
        "            # Append results for this question\n",
        "            group_results.append({\n",
        "                \"question\": question,\n",
        "                \"groundtruth\": ground_truth,\n",
        "                \"results\": model_results\n",
        "            })\n",
        "\n",
        "        results[group_name] = group_results\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Load evaluation dataset\n",
        "    with open(\"evaluation_results_organized.json\", 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    logging.info(\"Loaded JSON data successfully!\")\n",
        "\n",
        "    # Perform evaluation\n",
        "    evaluation_results = evaluate_models(data)\n",
        "\n",
        "    # Save results to JSON\n",
        "    with open(\"evaluation_results_final.json\", 'w', encoding='utf-8') as file:\n",
        "        json.dump(evaluation_results, file, indent=4)\n",
        "\n",
        "    logging.info(\"Evaluation complete! Results saved to 'evaluation_results_final.json'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uDRb4mowvOjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What defines arterial hypertension according to the National High Blood Pressure Education Programme (NHBPEP) for below 13 years?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 200, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ======== Load Models ========\n",
        "BASE_MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B\"\n",
        "MODEL_PATHS = {\n",
        "    \"Base Model\": None,  # Base model will be loaded from Hugging Face\n",
        "    \"BitFit\": \"./outputs/BitFit_gguf\",\n",
        "    \"LoRA\": \"./outputs/LoRA_gguf\",\n",
        "    \"QLoRA\": \"./outputs/QLoRA_gguf\"\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "# Define the Alpaca-style prompt template\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Input Prompt\n",
        "instruction = \"How should resistant hypertension be managed in children?\"\n",
        "input_text = \"\"\n",
        "\n",
        "# Tokenize input\n",
        "def tokenize_input(instruction, input_text):\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, \"\")\n",
        "    return tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate response function\n",
        "def generate_response(model, inputs):\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# Run inference for each model\n",
        "responses = {}\n",
        "\n",
        "for model_name, model_path in MODEL_PATHS.items():\n",
        "    print(f\"\\n=== Running inference with {model_name} ===\")\n",
        "\n",
        "    # Load base model or fine-tuned model\n",
        "    if model_path is None:\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=BASE_MODEL_NAME,\n",
        "            max_seq_length=1024,\n",
        "            dtype=torch.float16,\n",
        "            load_in_4bit=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "    else:\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=BASE_MODEL_NAME,\n",
        "            max_seq_length=1024,\n",
        "            dtype=torch.float16,\n",
        "            load_in_4bit=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model.load_adapter(model_path)  # Load LoRA/QLoRA/RS-LoRA adapters\n",
        "\n",
        "    # ✅ Ensure model is correctly referenced\n",
        "    FastLanguageModel.for_inference(model)  # Enable fast inference\n",
        "\n",
        "    # Prepare input\n",
        "    inputs = tokenize_input(instruction, input_text)\n",
        "\n",
        "    # Generate output\n",
        "    response = generate_response(model, inputs)\n",
        "\n",
        "    # Store response\n",
        "    responses[model_name] = response\n",
        "\n",
        "    # Print response\n",
        "    print(response)\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Print all responses together\n",
        "print(\"\\n=== Final Comparative Results ===\")\n",
        "for model_name, response in responses.items():\n",
        "    print(f\"\\n{model_name} Response:\\n{response}\")\n"
      ],
      "metadata": {
        "id": "I0veam3U_2n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Default Inference Scripts of Unsloth**"
      ],
      "metadata": {
        "id": "aLMyLPqZu-5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you've already loaded the model and tokenizer from your fine-tuning process\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained('/content/lora_model')  # Your saved output directory\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/lora_model')  # Your saved tokenizer\n",
        "\n",
        "# For faster inference with the custom model (if applicable)\n",
        "#FastLanguageModel.for_inference(model)  # Enable faster inference (optional if supported)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define a sample instruction and input from your dataset for testing\n",
        "instruction = \"A 10-year-old girl is presented for a blood pressure check in your practice. How do you proceed with the measurement?\"\n",
        "input_text = \"\"  # No input in this case or you can provide specific input\n",
        "output_text = \"\"  # Leave this blank for generation\n",
        "\n",
        "# Format the prompt\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\"\"\".format(instruction, input_text)\n",
        "\n",
        "# Tokenize the formatted prompt\n",
        "inputs = tokenizer(\n",
        "    [alpaca_prompt], return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
        ").to(\"cuda\")  # Assuming you're using GPU (cuda), otherwise use .to(\"cpu\")\n",
        "\n",
        "# Generate the response with adjusted parameters\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,  # Add some randomness to the generation\n",
        "    top_p=0.9,  # Use top-p sampling for better diversity\n",
        "    top_k=50,  # Limit the sampling pool for diversity\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"Generated Response: {generated_text[0]}\")"
      ],
      "metadata": {
        "id": "gEEXZREngVWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"A 10-year-old girl is presented for a blood pressure check in your practice. How do you proceed with the measurement?\", # instruction\n",
        "        \"First, ensure the girl rests and relaxes for 5 minutes. Then, use an age- and size-appropriate cuff, and measure the blood pressure on the upper arm. If unusual values occur, measure blood pressure in both arms and in the legs.\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}